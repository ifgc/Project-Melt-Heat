{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "91be548b",
   "metadata": {},
   "source": [
    "## Light Curve Procesing Hell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76f8ed4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "from astropy.io import fits\n",
    "from astropy.table import Table\n",
    "import pandas as pd\n",
    "MasterDF = pd.read_csv('MergedAPOGEEZTF_Data.csv')\n",
    "MasterDF.rename(columns={'ZTF_filtercode': 'filtercode'}, inplace=True)\n",
    "\n",
    "def extract_light_curve(file_path):\n",
    "    with fits.open(file_path) as hdul:\n",
    "        data = hdul[1].data\n",
    "        table = Table(data)\n",
    "    names = [name for name in table.colnames if len(table[name].shape) <= 1]\n",
    "    return table[names].to_pandas()\n",
    "\n",
    "def search_and_process_files(directory, keyword):\n",
    "    # Search for all FITS files containing the keyword in the filename\n",
    "    search_pattern = os.path.join(directory, f'*{keyword}*.fits')\n",
    "    files = glob.glob(search_pattern)\n",
    "    \n",
    "    # List to store each light curve DataFrame\n",
    "    all_light_curves = []\n",
    "    \n",
    "    # Process each file\n",
    "    for file in files:\n",
    "        try:\n",
    "            light_curve_df = extract_light_curve(file)\n",
    "            all_light_curves.append(light_curve_df)\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {file}: {e}\")\n",
    "\n",
    "    # Combine all light curves into a single DataFrame\n",
    "    if all_light_curves:\n",
    "        combined_df = pd.concat(all_light_curves, ignore_index=True)\n",
    "        return combined_df\n",
    "    else:\n",
    "        return pd.DataFrame()  # Return an empty DataFrame if no files were processed\n",
    "\n",
    "# Implementation\n",
    "directory = 'ZTF_LightCurves'\n",
    "keyword = 'APOGEE_ZTF_'\n",
    "combined_light_curves = search_and_process_files(directory, keyword)\n",
    "display(combined_light_curves.columns)\n",
    "#file is too large and doesnt save correctly, future update should implement saving to the csv in batches \n",
    "#combined_light_curves.to_csv('combined_light_curves.csv', index_label='Index', mode='w', header=True, index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f54cbc37",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Assuming above cells have  been run;\n",
    "# So ZTF_resultsColumns and combined_light_curves shoud be already defined DataFrames! \n",
    "# First, create a subset of ZTF_resultsColumns with only the relevant columns\n",
    "ztf_subset = MasterDF[['oid', 'source_id_01', 'apogee_id_01']]\n",
    "\n",
    "# Merge this subset with combined_light_curves\n",
    "# Using 'left' merge to keep every row in combined_light_curves and add matching data from ztf_subset\n",
    "combined_light_curves = pd.merge(combined_light_curves, ztf_subset, on='oid', how='left')\n",
    "\n",
    "# This operation will add the 'source_id_01' and 'apogee_id_01' columns to combined_light_curves\n",
    "display(combined_light_curves.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fee4f8fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_objects = combined_light_curves.drop_duplicates(subset=['apogee_id_01'], keep='first')\n",
    "print(len(unique_objects))\n",
    "unique_objects = combined_light_curves.drop_duplicates(subset=['source_id_01'], keep='first')\n",
    "print(len(unique_objects))\n",
    "unique_objects = combined_light_curves.drop(combined_light_curves[combined_light_curves['filtercode']!= 'zg'].index)\n",
    "unique_objects = unique_objects.drop_duplicates(subset=['ra'], keep='first')\n",
    "unique_objects = unique_objects.drop_duplicates(subset=['dec'], keep='first')\n",
    "unique_objects = unique_objects.drop_duplicates(subset=['oid'], keep='first')\n",
    "print(len(unique_objects))\n",
    "#OIDs = np.unique(unique_objects['oid'])\n",
    "print(combined_light_curves.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7419c9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(combined_light_curves['source_id_01'].unique().size)\n",
    "OIDs =combined_light_curves['source_id_01'].unique()\n",
    "print(len(OIDs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e63013a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from astropy.time import Time\n",
    "import matplotlib.dates as mdates\n",
    "from matplotlib.dates import DayLocator\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "def plotLightCurves(ObjNum):\n",
    "    test_oid = OIDs[ObjNum]\n",
    "    this_LC = combined_light_curves[combined_light_curves['source_id_01'] == test_oid]\n",
    "    print\n",
    "    # Check if the DataFrame is empty and handle it\n",
    "    if this_LC.empty:\n",
    "        print(f\"No data found for OID {test_oid}\")\n",
    "        return  # Exit the function if no data is found\n",
    "\n",
    "    filters = np.unique(this_LC['filtercode'])\n",
    "    filter_colors = {'zg': 'green', 'zr': 'red', 'zi': 'blue'}  # Define color for each filter\n",
    "\n",
    "    for filt in filters:\n",
    "        this_filter = this_LC[this_LC['filtercode'] == filt]\n",
    "        if this_filter.empty:\n",
    "            continue  # Skip to the next filter if no data is found\n",
    "\n",
    "        plt.figure(figsize=(16, 9))  # Create a new figure with a 16:9 aspect ratio for each filter\n",
    "\n",
    "        # Convert MJD to date format\n",
    "        mjd_dates = Time(this_filter['mjd'], format='mjd').datetime\n",
    "\n",
    "        # Plot the data points with error bars, using the specified color for each filter\n",
    "        color = filter_colors.get(filt, 'black')  # Default to black if filter color not specified\n",
    "        plt.errorbar(mjd_dates, this_filter['mag'], yerr=this_filter['magerr'], fmt='o', color=color, rasterized=True)\n",
    "\n",
    "        # Set x-axis labels to date format\n",
    "        plt.gca().xaxis.set_major_formatter(mdates.DateFormatter('%Y-%m-%d'))\n",
    "        plt.gcf().autofmt_xdate()  \n",
    "        plt.title(f\"ZTF Light Curve of Gaia EDR3 {test_oid}, Filter: {filt}\")\n",
    "        plt.gca().invert_yaxis()\n",
    "        #plt.show()  # Display the plot\n",
    "\n",
    "    # Print overall information outside the loop\n",
    "    print(f\"Plots completed for Object ID: Gaia EDR3 {test_oid}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e132d88",
   "metadata": {},
   "outputs": [],
   "source": [
    "for objectNum in range(200,250):\n",
    "    plotLightCurves(objectNum)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa4cc2d6",
   "metadata": {},
   "source": [
    "## Part 2?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8044ac15",
   "metadata": {},
   "outputs": [],
   "source": [
    "import feets\n",
    "import numpy as np\n",
    "import sys\n",
    "import warnings\n",
    "import pandas as pd\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "\n",
    "# Define feature extraction space and additional data columns\n",
    "features_to_extract =[\n",
    "    \"Amplitude\",\n",
    "    \"AndersonDarling\",\n",
    "    \"Autocor_length\",\n",
    "    \"Beyond1Std\",\n",
    "    \"CAR_mean\",\n",
    "    \"CAR_sigma\",\n",
    "    \"CAR_tau\",\n",
    "    \"Con\",\n",
    "    \"Eta_e\",\n",
    "    \"FluxPercentileRatioMid20\",\n",
    "    \"FluxPercentileRatioMid35\",\n",
    "    \"FluxPercentileRatioMid50\",\n",
    "    \"FluxPercentileRatioMid65\",\n",
    "    \"FluxPercentileRatioMid80\",\n",
    "    \"Freq1_harmonics_amplitude_0\",\n",
    "    \"Freq1_harmonics_amplitude_1\",\n",
    "    \"Freq1_harmonics_amplitude_2\",\n",
    "    \"Freq1_harmonics_amplitude_3\",\n",
    "    \"Freq1_harmonics_rel_phase_0\",\n",
    "    \"Freq1_harmonics_rel_phase_1\",\n",
    "    \"Freq1_harmonics_rel_phase_2\",\n",
    "    \"Freq1_harmonics_rel_phase_3\",\n",
    "    \"Freq2_harmonics_amplitude_0\",\n",
    "    \"Freq2_harmonics_amplitude_1\",\n",
    "    \"Freq2_harmonics_amplitude_2\",\n",
    "    \"Freq2_harmonics_amplitude_3\",\n",
    "    \"Freq2_harmonics_rel_phase_0\",\n",
    "    \"Freq2_harmonics_rel_phase_1\",\n",
    "    \"Freq2_harmonics_rel_phase_2\",\n",
    "    \"Freq2_harmonics_rel_phase_3\",\n",
    "    \"Freq3_harmonics_amplitude_0\",\n",
    "    \"Freq3_harmonics_amplitude_1\",\n",
    "    \"Freq3_harmonics_amplitude_2\",\n",
    "    \"Freq3_harmonics_amplitude_3\",\n",
    "    \"Freq3_harmonics_rel_phase_0\",\n",
    "    \"Freq3_harmonics_rel_phase_1\",\n",
    "    \"Freq3_harmonics_rel_phase_2\",\n",
    "    \"Freq3_harmonics_rel_phase_3\",\n",
    "    \"Gskew\",\n",
    "    \"LinearTrend\",\n",
    "    \"MaxSlope\",\n",
    "    \"Mean\",\n",
    "    \"Meanvariance\",\n",
    "    \"MedianAbsDev\",\n",
    "    \"MedianBRP\",\n",
    "    \"PairSlopeTrend\",\n",
    "    \"PercentAmplitude\",\n",
    "    \"PercentDifferenceFluxPercentile\",\n",
    "    \"PeriodLS\",\n",
    "    \"Period_fit\",\n",
    "    \"Psi_CS\",\n",
    "    \"Psi_eta\",\n",
    "    \"Q31\",\n",
    "    \"Rcs\",\n",
    "    \"Skew\",\n",
    "    \"SlottedA_length\",\n",
    "    \"SmallKurtosis\",\n",
    "    \"Std\",\n",
    "    \"StetsonK\",\n",
    "    \"StetsonK_AC\",\n",
    "    \"StructureFunction_index_21\",\n",
    "    \"StructureFunction_index_31\",\n",
    "    \"StructureFunction_index_32\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0750b189",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import pandas as pd\n",
    "import feets\n",
    "import os\n",
    "\n",
    "\n",
    "# Set up logging\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.INFO)\n",
    "\n",
    "output_dir = 'logs/2024-7-25'\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)\n",
    "\n",
    "# Create file handler which logs even debug messages\n",
    "fh = logging.FileHandler(f'{output_dir}/processing.log')\n",
    "fh.setLevel(logging.INFO)\n",
    "fh.setFormatter(logging.Formatter('%(asctime)s - %(levelname)s - %(message)s'))\n",
    "logger.addHandler(fh)\n",
    "\n",
    "# Create console handler with a higher log level\n",
    "ch = logging.StreamHandler()\n",
    "ch.setLevel(logging.INFO)\n",
    "ch.setFormatter(logging.Formatter('%(asctime)s - %(levelname)s - %(message)s'))\n",
    "logger.addHandler(ch)\n",
    "\n",
    "# Define columns for the failure log\n",
    "failure_columns = ['oid', 'filtercode', 'error_message', 'num_data_points']\n",
    "failures_df = pd.DataFrame(columns=failure_columns)\n",
    "\n",
    "columns = features_to_extract + ['apogee_id_01', 'filtercode', 'source_id_01', 'oid', 'ra', 'dec']\n",
    "initial_df = pd.DataFrame(columns=columns)\n",
    "initial_df.to_csv(f'{output_dir}/extracted_features.csv', index_label='Index', mode='w', header=True, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3bc69e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_and_extract_features(df, batch_size=10):\n",
    "    \"\"\"\n",
    "    Process and extract features from light curves, grouping by 'apogee_id_01'.\n",
    "    \n",
    "    Args:\n",
    "    df (DataFrame): The input DataFrame containing light curve data.\n",
    "    batch_size (int): The size of batches for writing to CSV files.\n",
    "    \n",
    "    Returns:\n",
    "    None\n",
    "    \"\"\"\n",
    "    # Initialize the feature extraction object with specific features to extract\n",
    "    fs = feets.FeatureSpace(only=features_to_extract)\n",
    "    \n",
    "    # Group the DataFrame by the 'apogee_id' column\n",
    "    grouped = df.groupby('apogee_id_01')\n",
    "    total_apogee_ids = len(grouped)  # Total number of unique 'apogee_id' groups\n",
    "    batch_data = []  # Initialize a list to store batch data\n",
    "    \n",
    "    # Iterate over the grouped data\n",
    "    for start_idx, (apogee_id, object_data) in enumerate(grouped):\n",
    "        # Extract relevant information from the first row of the group\n",
    "        ra = object_data['ra'].iloc[0]\n",
    "        dec = object_data['dec'].iloc[0]\n",
    "        source_id = object_data['source_id_01'].iloc[0]\n",
    "        oid = object_data['oid'].iloc[0]\n",
    "        \n",
    "        # Process data for each filter code\n",
    "        for filter_code in ['zg']:\n",
    "            filter_data = object_data[object_data['filtercode'] == filter_code]\n",
    "            \n",
    "            # Handle cases where there is insufficient data\n",
    "            if filter_data.empty:\n",
    "                failures_df.loc[len(failures_df)] = [apogee_id, filter_code, 'Insufficient data points', 0]\n",
    "                logger.info(\"Filter Data empty\")\n",
    "                logger.warning(f\"Filter Data Empty for {apogee_id} Source:{source_id} and filter {filter_code}: Insufficient data points: 0\")\n",
    "                continue\n",
    "\n",
    "            times = filter_data['mjd'].values\n",
    "            magnitudes = filter_data['mag'].values\n",
    "            errors = filter_data['magerr'].values\n",
    "\n",
    "            if len(filter_data) < 100:\n",
    "                failures_df.loc[len(failures_df)] = [apogee_id, filter_code, 'Insufficient data points', len(filter_data)]\n",
    "                logger.warning(f\"Dropped APOGEE_ID {apogee_id} Source:{source_id} and filter {filter_code}: Insufficient data points:{len(filter_data)}\")\n",
    "                continue\n",
    "                \n",
    "            try:\n",
    "                # Extract features using the specified feature extraction object\n",
    "                features_values, feature_names = fs.extract(time=times, magnitude=magnitudes, error=errors)\n",
    "                row = dict(zip(features_values, feature_names))\n",
    "                row.update({'apogee_id': apogee_id, 'filtercode': filter_code, 'source_id_01': source_id, 'oid': oid, 'ra': ra, 'dec': dec})\n",
    "                batch_data.append(row)\n",
    "            except Exception as e:\n",
    "                failures_df.loc[len(failures_df)] = [apogee_id, filter_code, str(e), len(filter_data)]\n",
    "                logger.error(f\"Exception for APOGEE_ID {apogee_id}: {e}\")\n",
    "\n",
    "        # Write batch data to CSV when batch size is reached or all groups are processed\n",
    "        if len(batch_data) >= batch_size or (start_idx == total_apogee_ids - 1):\n",
    "            logger.info(f\"Writing batch: current size = {len(batch_data)}\")\n",
    "            pd.DataFrame(batch_data).to_csv(f'{output_dir}/extracted_features.csv', mode='a', header=False, index=False)\n",
    "            batch_data = []\n",
    "\n",
    "        # Log progress\n",
    "        progress = (start_idx + 1) / total_apogee_ids * 100\n",
    "        logger.info(f\"\\rProcessing GaiaDR3{source_id} {start_idx + 1}/{total_apogee_ids} APOGEE_IDs ({progress:.2f}%) with batch size {len(batch_data)}\")\n",
    "\n",
    "    logger.info(\"Feature extraction complete. Data saved to 'extracted_features.csv'.\")\n",
    "    failures_df.to_csv(f'{output_dir}/extraction_failures.csv', index=False)\n",
    "    logger.info(\"Failures logged in 'extraction_failures.csv'.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07c74dc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage:\n",
    "# combined_light_curves = pd.read_csv('your_input_file.csv')\n",
    "# process_and_extract_features(combined_light_curves)\n",
    "\n",
    "\n",
    "# look into pdb trace\n",
    "\n",
    "#usage\n",
    "process_and_extract_features(combined_light_curves)\n",
    "\n",
    "lost_light_curves = combined_light_curves[combined_light_curves.index < len(pd.read_csv(f'{output_dir}/2024-7-25/extracted_features.csv'))]\n",
    "print(len(lost_light_curves['oid'].unique()))\n",
    "print(len(combined_light_curves['oid'].unique()))\n",
    "print(len(combined_light_curves['oid'].unique()) - len(lost_light_curves['oid'].unique()))\n",
    "#process_and_extract_features(lost_light_curves)\n",
    "\n",
    "process_and_extract_features(lost_light_curves)\n",
    "\n",
    "print(len(combined_light_curves['oid'].unique()))\n",
    "print(len(pd.read_csv(f'{output_dir}/2024-7-25/extracted_features.csv')))\n",
    "print(len(failures_df))\n",
    "\n",
    "\n",
    "display(pd.read_csv(f'{output_dir}/2024-7-25/extracted_features.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d97b82cd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf393a7f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (project_meltheat_v2)",
   "language": "python",
   "name": "project_meltheat_v2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
